# -*- coding: utf-8 -*-
"""HRME-DEC.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-t5gSPddMfGrzIIWKaMH4xQgXUziGI4J
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, random_split, ConcatDataset, Subset
import numpy as np
import copy

from tqdm import tqdm

from sklearn.metrics import confusion_matrix, classification_report


transform_train = transforms.Compose([
    transforms.RandomResizedCrop((224, 224), scale=(0.8, 1.0)),
    transforms.RandomHorizontalFlip(p=0.5),
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                         std=[0.229, 0.224, 0.225])
])

transform_val = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                         std=[0.229, 0.224, 0.225])
])

class SqueezeBlockTranspose(nn.Module):

    def __init__(self, in_channels, out_channels, stride=1, groups=1, dropout_p=0.1):
        super(SqueezeBlockTranspose, self).__init__()
        self.stride = stride
        self.groups = groups

        self.conv1 = nn.ConvTranspose2d(
            in_channels, out_channels, kernel_size=3,
            stride=stride, padding=1, output_padding=(stride - 1),
            groups=groups, bias=False
        )
        self.bn1 = nn.BatchNorm2d(out_channels)

        self.conv2 = nn.ConvTranspose2d(
            out_channels, out_channels, kernel_size=3,
            stride=1, padding=1, output_padding=0,
            groups=groups, bias=False
        )
        self.bn2 = nn.BatchNorm2d(out_channels)

        self.skip = nn.Sequential()
        if stride != 1 or in_channels != out_channels:
            self.skip = nn.Sequential(
                nn.ConvTranspose2d(
                    in_channels, out_channels, kernel_size=1,
                    stride=stride, output_padding=stride-1,
                    groups=groups, bias=False
                ),
                nn.BatchNorm2d(out_channels)
            )

        self.dropout = nn.Dropout2d(p=dropout_p)
        self.relu = nn.ReLU(inplace=True)

    def forward(self, x):
        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)

        out = self.conv2(out)
        out = self.bn2(out)

        shortcut = self.skip(x)
        out += shortcut
        out = self.relu(out)

        out = self.dropout(out)
        return out


class SqueezeNetDecoder(nn.Module):

    def __init__(self, start_channels=512, dropout_p=0.1):
        super(SqueezeNetDecoder, self).__init__()

        self.stage1 = self._make_stage(start_channels, 256, stride=2, dropout_p=dropout_p)  # 14->28
        self.stage2 = self._make_stage(256, 128, stride=2, dropout_p=dropout_p)            # 28->56
        self.stage3 = self._make_stage(128, 64,  stride=2, dropout_p=dropout_p)            # 56->112
        self.stage4 = self._make_stage(64, 64,   stride=2, dropout_p=dropout_p)            #112->224

        self.conv_last = nn.ConvTranspose2d(
            64, 3, kernel_size=3, stride=1, padding=1, output_padding=0
        )
        self.sigmoid = nn.Sigmoid()

    def _make_stage(self, in_c, out_c, stride=2, groups=1, dropout_p=0.1):
        return nn.Sequential(
            SqueezeBlockTranspose(in_c, out_c, stride=stride, groups=groups, dropout_p=dropout_p),
            SqueezeBlockTranspose(out_c, out_c, stride=1, groups=groups, dropout_p=dropout_p)
        )

    def forward(self, x):
        out = self.stage1(x)
        out = self.stage2(out)
        out = self.stage3(out)
        out = self.stage4(out)
        out = self.conv_last(out)
        out = self.sigmoid(out)
        return out

class CenterLoss(nn.Module):
    def __init__(self, num_classes, feat_dim):
        super(CenterLoss, self).__init__()
        self.centers = nn.Parameter(torch.randn(num_classes, feat_dim))

    def forward(self, features, labels):

        if labels.max() >= self.centers.size(0) or labels.min() < 0:
            raise ValueError(f"Labels out of range. Labels should be between 0 and {self.centers.size(0)-1}.")
        centers_batch = self.centers.index_select(0, labels)
        return nn.MSELoss()(features, centers_batch)

class FocalLoss(nn.Module):
    def __init__(self, alpha=1.0, gamma=2.0, reduction='mean'):
        super(FocalLoss, self).__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction
        self.ce = nn.CrossEntropyLoss(reduction='none')

    def forward(self, logits, targets):
        ce_loss = self.ce(logits, targets)
        pt = torch.exp(-ce_loss)
        focal_loss = self.alpha * (1 - pt)**self.gamma * ce_loss

        if self.reduction == 'mean':
            return focal_loss.mean()
        elif self.reduction == 'sum':
            return focal_loss.sum()
        return focal_loss

class SqueezeNetFullAutoencoder(nn.Module):

    def __init__(self, embedding_dim, n_clusters):
        super(SqueezeNetFullAutoencoder, self).__init__()
        base = torchvision.models.squeezenet1_0(pretrained=True)

        self.encoder = base.features

        self.globalpool = nn.AdaptiveAvgPool2d((1,1))

        self.embedding_fc = nn.Linear(512, embedding_dim)

        self.decoder_fc = nn.Linear(embedding_dim, 512*14*14)
        self.decoder_net = SqueezeNetDecoder(start_channels=512, dropout_p=0.1)

        self.n_clusters = n_clusters
        self.cluster_centers = nn.Parameter(torch.randn(n_clusters, embedding_dim))

        self.center_loss_fn = CenterLoss(num_classes=n_clusters, feat_dim=embedding_dim)

    def forward(self, x, labels=None):
        feats = self.encoder(x)  # (N,512,14,14)
        feats = self.globalpool(feats)  # (N,512,1,1)
        feats = feats.view(feats.size(0), -1)  # (N,512)
        embedded = self.embedding_fc(feats)    # (N,embedding_dim)

        dec_input = self.decoder_fc(embedded)
        dec_input = dec_input.view(-1, 512, 14, 14)
        reconstructed = self.decoder_net(dec_input)

        center_loss_val = None
        if labels is not None:
            center_loss_val = self.center_loss_fn(embedded, labels)

        return reconstructed, embedded, center_loss_val

    def cluster_prob(self, embedded):

        batch_size = embedded.size(0)
        e_emb = embedded.unsqueeze(1).expand(batch_size, self.n_clusters, -1)
        e_centers = self.cluster_centers.unsqueeze(0).expand(batch_size, self.n_clusters, -1)
        dist = (e_emb - e_centers).pow(2).sum(dim=2)
        cluster_probs = torch.softmax(-dist, dim=1)
        return cluster_probs


class Classifier(nn.Module):
    def __init__(self, embedding_dim, num_classes=10, dropout_p=0.1):
        super(Classifier, self).__init__()
        self.net = nn.Sequential(
            nn.Linear(embedding_dim, 256),
            nn.BatchNorm1d(256),
            nn.ReLU(),
            nn.Dropout(dropout_p),
            nn.Linear(256, 128),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.Dropout(dropout_p),
            nn.Linear(128, num_classes)
        )

    def forward(self, x):
        return self.net(x)

class EarlyStopping:
    def __init__(self, patience=3, delta=1e-4, path='checkpoint.pth'):
        self.patience = patience
        self.delta = delta
        self.path = path
        self.counter = 0
        self.best_loss = None
        self.early_stop = False

    def __call__(self, val_loss, model):
        if self.best_loss is None:
            self.best_loss = val_loss
            self.save_checkpoint(model)
        elif val_loss > self.best_loss - self.delta:
            self.counter += 1
            print(f"EarlyStopping counter: {self.counter} out of {self.patience}")
            if self.counter >= self.patience:
                self.early_stop = True
        else:
            self.best_loss = val_loss
            self.save_checkpoint(model)
            self.counter = 0

    def save_checkpoint(self, model):
        torch.save(model.state_dict(), self.path)

def GROW_TREE(
    model,
    classifier,
    data_loader,
    device,
    depth,
    max_depth,
    k_min,
    k_max,
    cluster_min_size
):
    """
    Stopping Criteria:
        - Max depth exceeded
        - Any cluster has size below cluster_min_size
    """
    class Node:
        def __init__(self):
            self.is_leaf = False
            self.model = None
            self.classifier = None
            self.children = {}

    node = Node()
    dataset_size = len(data_loader.dataset)

    if depth >= max_depth:
        node.is_leaf = True
        node.model = model
        node.classifier = classifier
        print(f"[Leaf Node] Depth={depth}, Samples={dataset_size}")
        return node

    print(f"[Branch Node] Depth={depth}, Samples={dataset_size}")

    # E-step:
    embeddings_list, labels_list, images_list = [], [], []
    for batch in data_loader:
        imgs, labs = batch
        imgs = imgs.to(device)
        labs = labs.to(device)
        with torch.no_grad():
            _, emb, _ = model(imgs)
        embeddings_list.append(emb.cpu())
        labels_list.append(labs.cpu())
        images_list.append(imgs.cpu())

    all_emb = torch.cat(embeddings_list, dim=0)  # (N, embedding_dim)
    all_labels = torch.cat(labels_list, dim=0)   # (N,)
    all_imgs = torch.cat(images_list, dim=0)     # (N, 3, 224, 224)

    best_k = None
    best_weighted_acc = -1.0

    from copy import deepcopy
    focal_loss_fn = FocalLoss(alpha=1.0, gamma=2.0, reduction='mean')
    mse_loss_fn = nn.MSELoss()

    print(f"Searching for optimal K between {k_min} and {k_max}...")
    for k in tqdm(range(k_min, k_max+1), desc="K-Value Search"):
        print(f"\nEvaluating K={k}...")
        temp_model = deepcopy(model)
        temp_classifier = deepcopy(classifier)
        temp_model.n_clusters = k
        with torch.no_grad():
            temp_model.cluster_centers.data = torch.randn(k, temp_model.embedding_fc.out_features).to(device)

        temp_optimizer = optim.Adam(list(temp_model.parameters()) + list(temp_classifier.parameters()),
                                    lr=1e-4, weight_decay=1e-5)
        epochs_k = 5

        for epoch_k in tqdm(range(epochs_k), desc=f"Training for K={k}", leave=False):
            temp_model.train()
            temp_classifier.train()
            for batch_k in data_loader:
                imgs_k, labs_k = batch_k
                imgs_k, labs_k = imgs_k.to(device), labs_k.to(device)
                temp_optimizer.zero_grad()
                recon_k, emb_k, _ = temp_model(imgs_k, labels=None)
                logits_k = temp_classifier(emb_k)

                loss_recon_k = mse_loss_fn(recon_k, imgs_k)
                loss_class_k = focal_loss_fn(logits_k, labs_k)
                total_loss_k = loss_recon_k + loss_class_k
                total_loss_k.backward()
                temp_optimizer.step()

        temp_model.eval()
        with torch.no_grad():
            emb_device = all_emb.to(device)
            cluster_probs = temp_model.cluster_prob(emb_device).cpu()  # (N, k)
        cluster_assignments_k = torch.argmax(cluster_probs, dim=1)  # (N,)

        unique_k, counts_k = torch.unique(cluster_assignments_k, return_counts=True)
        if (counts_k < cluster_min_size).any():
            small_clusters = unique_k[counts_k < cluster_min_size]
            print(f"Skipping K={k} because clusters {small_clusters.tolist()} have fewer than {cluster_min_size} samples.")
            continue
        else:
            print(f"All clusters in K={k} meet the minimum size requirement.")

        # M - Step
        weighted_acc = 0.0
        total_samples_k = len(cluster_assignments_k)
        for c in tqdm(range(k), desc=f"Evaluating Cluster {k}", leave=False):
            idx_c = (cluster_assignments_k == c).nonzero(as_tuple=True)[0]
            c_count = idx_c.size(0)

            subset_emb = all_emb[idx_c].to(device)
            subset_labels = all_labels[idx_c].to(device)

            temp_classifier_c = deepcopy(temp_classifier)
            temp_classifier_c.train()
            optimizer_c = optim.Adam(temp_classifier_c.parameters(), lr=1e-4, weight_decay=1e-5)
            epochs_c = 3

            for epoch_c in tqdm(range(epochs_c), desc=f"Training Classifier for Cluster {c}", leave=False):
                optimizer_c.zero_grad()
                logits_c = temp_classifier_c(subset_emb)
                loss_c = focal_loss_fn(logits_c, subset_labels)
                loss_c.backward()
                optimizer_c.step()

            temp_classifier_c.eval()
            with torch.no_grad():
                logits_pred_c = temp_classifier_c(subset_emb)
                preds_c = torch.argmax(logits_pred_c, dim=1)
                acc_c = (preds_c == subset_labels).float().mean().item()
                weighted_acc += (c_count / total_samples_k) * acc_c

        print(f"K={k}, Weighted Accuracy={weighted_acc:.4f}")

        if weighted_acc > best_weighted_acc:
            best_weighted_acc = weighted_acc
            best_k = k

    if best_k is None:
        print("No valid K found within the specified range that meets the cluster size requirement.")
        node.is_leaf = True
        node.model = model
        node.classifier = classifier
        return node

    print(f"\nOptimal K found: {best_k} with Weighted Accuracy={best_weighted_acc:.4f}")

    model.n_clusters = best_k
    with torch.no_grad():
        model.cluster_centers.data = torch.randn(best_k, model.embedding_fc.out_features).to(device)

    model.eval()
    with torch.no_grad():
        emb_device = all_emb.to(device)
        cluster_probs = model.cluster_prob(emb_device).cpu()
    cluster_assignments = torch.argmax(cluster_probs, dim=1)

    unique, counts = torch.unique(cluster_assignments, return_counts=True)
    if (counts < cluster_min_size).any():
        small_clusters = unique[counts < cluster_min_size]
        print(f"Clusters {small_clusters.tolist()} have fewer than {cluster_min_size} samples. Stopping recursion.")
        node.is_leaf = True
        node.model = model
        node.classifier = classifier
        return node

    for c in tqdm(range(best_k), desc="Creating Child Nodes"):
        c_count = (cluster_assignments == c).sum().item()
        if c_count < cluster_min_size:
            print(f"Cluster {c} has {c_count} samples < cluster_min_size => skipping.")
            continue

        idx_c = (cluster_assignments == c).nonzero(as_tuple=True)[0]
        print(f"Creating child node for Cluster {c} with {c_count} samples. Depth={depth+1}")

        child_subset = Subset(data_loader.dataset, idx_c.tolist())
        child_loader = DataLoader(child_subset, batch_size=8, shuffle=True, num_workers=4, pin_memory=True)

        child_model = copy.deepcopy(model)
        child_classifier = copy.deepcopy(classifier)

        with torch.no_grad():
            child_model.cluster_centers.data = torch.randn(best_k, child_model.embedding_fc.out_features).to(device)

        child_model.eval()
        with torch.no_grad():
            child_embeddings = []
            for child_batch in child_loader:
                imgs_child, _ = child_batch
                imgs_child = imgs_child.to(device)
                _, emb_child, _ = child_model(imgs_child, labels=None)
                child_embeddings.append(emb_child.cpu())
            child_all_emb = torch.cat(child_embeddings, dim=0)
            child_cluster_probs = child_model.cluster_prob(child_all_emb.to(device)).cpu()
            child_cluster_assignments = torch.argmax(child_cluster_probs, dim=1)

        child_unique, child_counts = torch.unique(child_cluster_assignments, return_counts=True)
        if (child_counts < cluster_min_size).any():
            small_child_clusters = child_unique[child_counts < cluster_min_size]
            print(f"Child node Cluster {c} contains sub-clusters {small_child_clusters.tolist()} with fewer than {cluster_min_size} samples. Making it a leaf node.")
            child_node = Node()
            child_node.is_leaf = True
            child_node.model = child_model
            child_node.classifier = child_classifier
            node.children[c] = child_node
            continue

        child_node = GROW_TREE(
            child_model, child_classifier, child_loader, device, depth + 1, max_depth,
            k_min, k_max, cluster_min_size
        )
        node.children[c] = child_node

    return node

def domain_specific_hmoe():
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")

    ds_train = torchvision.datasets.CIFAR10(
        root='./data', train=True, download=True, transform=transform_train
    )
    ds_test = torchvision.datasets.CIFAR10(
        root='./data', train=False, download=True, transform=transform_train
    )
    full_dataset = ConcatDataset([ds_train, ds_test])
    full_size = len(full_dataset)
    print("Full domain dataset size (train + test):", full_size)


    original_train_size = len(ds_train)

    #Set Min Cluster Siz
    cluster_min_percent = 0.1  # 10%


    cluster_min_size = int(cluster_min_percent * original_train_size)
    print(f"Cluster minimum size (20% of original train size): {cluster_min_size}")

    val_ratio = 0.1
    val_size = int(full_size * val_ratio)
    train_size = full_size - val_size
    train_subset, val_subset = random_split(full_dataset, [train_size, val_size])
    train_loader = DataLoader(train_subset, batch_size=8, shuffle=True, num_workers=4, pin_memory=True)
    val_loader = DataLoader(val_subset, batch_size=8, shuffle=False, num_workers=4, pin_memory=True)

    #Pretrain  & DEC embed parameters
    embedding_dim = 256
    initial_k = 3


    model = SqueezeNetFullAutoencoder(embedding_dim, n_clusters=initial_k).to(device)
    classifier = Classifier(embedding_dim=embedding_dim, num_classes=10, dropout_p=0.1).to(device)

    params_to_optimize = list(model.parameters()) + list(classifier.parameters())
    optimizer = optim.Adam(params_to_optimize, lr=1e-4, weight_decay=1e-5)
    focal_loss_fn = FocalLoss(alpha=1.0, gamma=2.0, reduction='mean')
    mse_loss_fn = nn.MSELoss()
    early_stopper = EarlyStopping(patience=5, delta=1e-4, path='./best_hmoe_model.pth')

    epochs = 20
    for epoch in tqdm(range(epochs), desc="Pretraining Epochs"):
        model.train()
        classifier.train()
        epoch_loss = 0.0

        for batch in train_loader:
            imgs, labs = batch
            imgs = imgs.to(device)
            labs = labs.to(device)

            recon, emb, _ = model(imgs, labels=None)

            logits = classifier(emb)

            loss_recon = mse_loss_fn(recon, imgs)

            loss_class = focal_loss_fn(logits, labs)

            total_loss = loss_recon + loss_class
            total_loss.backward()
            optimizer.step()
            optimizer.zero_grad()

            epoch_loss += total_loss.item() * imgs.size(0)

        epoch_loss /= len(train_loader.dataset)

        val_loss = 0.0
        correct_val, total_val = 0, 0
        model.eval()
        classifier.eval()
        with torch.no_grad():
            for batch_val in val_loader:
                imgs_val, labs_val = batch_val
                imgs_val = imgs_val.to(device)
                labs_val = labs_val.to(device)
                recon_val, emb_val, _ = model(imgs_val, labels=None)
                logits_val = classifier(emb_val)

                loss_recon_val = mse_loss_fn(recon_val, imgs_val)

                loss_class_val = focal_loss_fn(logits_val, labs_val)
                total_val_batch = loss_recon_val + loss_class_val
                val_loss += total_val_batch.item() * imgs_val.size(0)

                preds_val = torch.argmax(logits_val, dim=1)
                correct_val += (preds_val == labs_val).sum().item()
                total_val += labs_val.size(0)

        val_loss /= len(val_loader.dataset)
        val_acc = correct_val / total_val
        print(f"Epoch [{epoch+1}/{epochs}] | Train Loss: {epoch_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}")
        early_stopper(val_loss, model)
        if early_stopper.early_stop:
            print("Early stopping triggered.")
            break

    model.load_state_dict(torch.load('best_hmoe_model.pth'))
    print("Loaded best HMoE model (SqueezeNet) from checkpoint.")

    final_loader = DataLoader(full_dataset, batch_size=8, shuffle=False, num_workers=4, pin_memory=True)


    # Params
    max_depth = 3
    k_min, k_max = 2, 5



    root_node = GROW_TREE(
        model, classifier, final_loader, device, depth=0, max_depth=max_depth,
        k_min=k_min, k_max=k_max, cluster_min_size=cluster_min_size
    )

    return root_node

def PREDICT(tree_node, image, device):

    image = image.unsqueeze(0).to(device)

    current_node = tree_node
    while not current_node.is_leaf:
        # Embed
        with torch.no_grad():
            _, emb, _ = current_node.model(image)
            probs = current_node.model.cluster_prob(emb)
        c_assignment = torch.argmax(probs, dim=1).item()

        if c_assignment in current_node.children:
            current_node = current_node.children[c_assignment]
        else:
            break

    with torch.no_grad():
        _, emb2, _ = current_node.model(image)
        logits = current_node.classifier(emb2)
        pred_label = torch.argmax(logits, dim=1).item()
    return pred_label

def evaluate_model_on_test(tree, test_loader, device):

    preds = []
    true_labels = []

    for batch in tqdm(test_loader, desc="Evaluating Test Set"):
        images, labels = batch
        for i in range(images.size(0)):
            img = images[i].unsqueeze(0)  # (1,3,224,224)
            pred = PREDICT(tree, img, device)
            preds.append(pred)
            true_labels.append(labels[i].item())

    preds = np.array(preds)
    true_labels = np.array(true_labels)

    accuracy = (preds == true_labels).mean()
    print(f"\nTest Accuracy: {accuracy:.4f}")

    cm = confusion_matrix(true_labels, preds)
    cr = classification_report(true_labels, preds)

    print("\nConfusion Matrix:\n", cm)
    print("\nClassification Report:\n", cr)

def MAIN():
    tree = domain_specific_hmoe()
    print("\nrecursisve EM training done")

    transform_test = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406],
                             std=[0.229, 0.224, 0.225])
    ])
    test_dataset = torchvision.datasets.CIFAR10(
        root='./data', train=False, download=True, transform=transform_test
    )
    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True, num_workers=4, pin_memory=True)

    evaluate_model_on_test(tree, test_loader, torch.device('cuda' if torch.cuda.is_available() else 'cpu'))

    return tree

if __name__ == '__main__':
    final_tree = MAIN()